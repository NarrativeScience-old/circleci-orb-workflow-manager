commands:
  cancel-job:
    description: Cancel the current job based on an environment variable
    parameters:
      method:
        default: cancel
        description: |
          Method of cancellation; Either cancel the job or just halt the step so the job
          still succeeds
        enum:
        - cancel
        - halt
        type: enum
    steps:
    - run:
        command: |
          echo "CANCEL_JOB=$CANCEL_JOB"
          [[ -z "$CANCEL_JOB" ]] && exit 0

          set -e

          if [[ "<< parameters.method >>" == "cancel" ]]; then
            OUTPUT=$(
              curl \
                --user "${CIRCLE_API_USER_TOKEN}:" \
                -X POST \
                --max-time 60 \
                --connect-timeout 60 \
                "https://circleci.com/api/v1.1/project/github/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}/${CIRCLE_BUILD_NUM}/cancel")
            echo "$OUTPUT"

            STATUS="$(echo "$OUTPUT" | jq -r .status -)"
            if [[ "$STATUS" == 'canceled' ]]; then
              # This means the job was cancelled but for some reason the current script is
              # still running. Wait a few seconds to let it catch up then fail the job to
              # prevent downstream jobs from running unintentionally.
              sleep 10
              exit 1
            fi

            echo "Failed to cancel job"
            exit 1

          elif [[ "<< parameters.method >>" == "halt" ]]; then
            # Halt the job and mark it as successful
            circleci step halt

          else
            echo "Unknown cancel method: << parameters.method >>"
            exit 1
          fi
        name: Cancel the job
  exit-queue:
    description: Exit the queue when a workflow is finished
    parameters:
      exit_condition:
        default: always
        description: Only release the lock under a certain condition
        enum:
        - on_success
        - on_fail
        - on_cancelled
        - always
        type: enum
      lock_key_env_var_name:
        default: WORKFLOW_LOCK_KEY
        description: |
          Environment variable containing the key of the workflow lock to release. This
          should generally be set in a Context.
        type: env_var_name
    steps:
    - attach_workspace:
        at: /tmp/workspace
    - run:
        command: |
          if [[ -z "$WORKFLOW_LOCK_BUILD_STATUS" ]]; then
            echo 'export WORKFLOW_LOCK_BUILD_STATUS=FAILED' >> $BASH_ENV
          fi
        name: Set failure condition
        when: on_fail
    - run:
        command: |
          if [[ -z "$WORKFLOW_LOCK_BUILD_STATUS" ]]; then
            echo 'export WORKFLOW_LOCK_BUILD_STATUS=SUCCESS' >> $BASH_ENV
          fi
        name: Set success condition
        when: on_success
    - run:
        command: |
          LOCK_KEY="${<< parameters.lock_key_env_var_name >>}"
          if [[ -z "$LOCK_KEY" ]]; then
            echo "No lock key set. Continuing..."
            exit 0
          fi

          <<# parameters.exit_condition >>
          if [[ \
            ("<< parameters.exit_condition >>" == "on_fail" \
              && "$WORKFLOW_LOCK_BUILD_STATUS" != "FAILED") \
            || ("<< parameters.exit_condition >>" == "on_success" \
              && "$WORKFLOW_LOCK_BUILD_STATUS" != "SUCCESS") \
            || ("<< parameters.exit_condition >>" == "on_cancelled" \
              && "$WORKFLOW_LOCK_BUILD_STATUS" != "CANCELLED") \
          ]]; then
            echo "Release condition is << parameters.exit_condition >> but status is ${WORKFLOW_LOCK_BUILD_STATUS}. Will not release lock for key ${LOCK_KEY}."
            exit 0
          fi
          <</ parameters.exit_condition >>

          # Update the status of the workflow and return the updated item
          # The key object is stored in /tmp/workspace/workflow-key.json
          VALUES="$(
            echo '{}' \
              | jq --arg s "$WORKFLOW_LOCK_BUILD_STATUS" '.[":status"].S = $s' - \
              | jq --arg t "$(date +%s)" '.[":released_at"].N = $t' -)"
          aws dynamodb update-item \
            --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
            --key "file:///tmp/workspace/workflow-key.json" \
            --update-expression 'SET #status = :status, #released_at = :released_at' \
            --expression-attribute-names '{"#status": "status", "#released_at": "released_at"}' \
            --expression-attribute-values "$VALUES" \
            --return-values ALL_NEW

          echo "Successfully released lock for $LOCK_KEY"
        name: Exit the workflow queue
        when: always
  initialize-state-store:
    description: Download state data to a local file and inject values into the environment
    parameters:
      path:
        default: /tmp/state.json
        description: Path to store the state file
        type: string
    steps:
    - attach_workspace:
        at: /tmp/workspace
    - run:
        command: |
          WORKFLOW_KEY_FILE=/tmp/workspace/workflow-key.json
          if [[ ! -f "$WORKFLOW_KEY_FILE" ]]; then
            echo "Workflow key file does not exist at ${WORKFLOW_KEY_FILE}. Skipping the initialization of the local state store..."
            exit 0
          fi

          # Get the state item
          ITEM="$(
            aws dynamodb get-item \
              --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
              --key "file://${WORKFLOW_KEY_FILE}" \
              --consistent-read)"

          # If the item doesn't exist, default to an empty item
          [[ -z "$ITEM" ]] && ITEM='{"Item": {}}'

          # Transform the state into a flat object and store on disk
          echo "$ITEM" | jq '.Item.state.M | map_values(values[])' - > "<< parameters.path >>"

          # Load the state values into the environment
          cat "<< parameters.path >>" \
            | jq -r 'to_entries | map("export \(.key)=\(.value | @sh)") | join("\n")' - \
            >> $BASH_ENV

          cat "<< parameters.path >>"
        name: Download state data to a local file
  send-slack-on-workflow-recovery:
    description: Send a Slack message to a channel if the workflow has recovered
    steps:
    - run:
        command: |
          values="$(
            echo '{":key": {"S": ""}, ":success": {"S": "SUCCESS"}, ":failed": {"S": "FAILED"}}' \
            | jq --arg k "$WORKFLOW_LOCK_KEY" '.[":key"].S = $k' -)"

          # Retrieves the statuses for the last two workflows with statuses
          # of either `SUCCESS` or `FAILURE`. The most recent value should be the
          # status of the current workflow after it has exited the queue
          # and updated its status on the corresponding dynamodb table.
          deploys="$(aws dynamodb query \
            --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
            --key-condition-expression '#key = :key' \
            --select "SPECIFIC_ATTRIBUTES" \
            --projection-expression "#status" \
            --filter-expression '#status IN (:success, :failed)' \
            --expression-attribute-names '{"#key": "key", "#status": "status"}' \
            --expression-attribute-values "$values" \
            --max-items 2 \
            --no-scan-index-forward | jq -r .Items)"

          current_deploy="$(echo "$deploys" | jq -r .[0].status.S)"
          prev_deploy="$(echo "$deploys" | jq -r .[1].status.S)"

          if [[ "$current_deploy" == "SUCCESS" && "$prev_deploy" == "FAILED" ]]; then
            :  # Noop to continue to next step to notify Slack
          else
            # Halt the job and mark it as successful
            circleci step halt
          fi
        name: Check if the workflow has recovered
    - slack/notify:
        color: '#1CBF43'
        include_job_number_field: false
        include_project_field: false
        include_visit_job_action: false
        message: ':tada: $WORKFLOW_LOCK_KEY has recovered!'
  set-state:
    description: Set state data to the remote store
    parameters:
      key:
        description: Key to set in the state
        type: string
      value_type:
        default: S
        description: |
          DynamoDB data type. See https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_AttributeValue.html.
          Only a few are currently supported in this command.
        enum:
        - S
        - "N"
        - BOOL
        - "NULL"
        type: enum
      value_var_name:
        description: Environment variable to source the value
        type: env_var_name
    steps:
    - run:
        command: |
          # If the value type is S (string) or N (number) then wrap the value in quotes
          # to feed to --argjson. Otherwise, we'll get a `Invalid type for parameter`
          # error.
          VALUE="${<< parameters.value_var_name >>}"
          if [[ "<< parameters.value_type >>" == "S" || "<< parameters.value_type >>" == "N" ]]; then
            VALUE="\"$VALUE\""
          fi

          # Create a temporary file containing expression attribute values (JSON)
          VALUES="$(mktemp)"
          echo '{}' \
            | jq --argjson v "$VALUE" '.[":value"]["<< parameters.value_type >>"] = $v' - \
            > "$VALUES"
          cat "$VALUES"

          # Update the state item and return the updated item
          # The key object is stored in /tmp/workspace/workflow-key.json
          aws dynamodb update-item \
            --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
            --key "file:///tmp/workspace/workflow-key.json" \
            --update-expression 'SET #state.#key = :value' \
            --expression-attribute-names '{"#state": "state", "#key": "<< parameters.key >>"}' \
            --expression-attribute-values "file://$VALUES" \
            --return-values ALL_NEW
        name: Set state data to the store (<< parameters.key >>)
  wait-in-queue:
    description: Wait in a workflow queue until the job is at the front
    parameters:
      check_previous_commit:
        default: false
        description: |
          Whether to check if the previous commit has been added to the queue before
          continuing with the workflow. This helps deal with the race condition of two
          commits getting merged seconds apart.
        type: boolean
      do_not_cancel_workflow_if_tag_in_commit:
        default: ""
        description: |
          Do not allow this workflow to self-cancel, even if it could have been ignored, if
          the provided tag is in the commit message (case-insensitive)
        type: string
      force:
        default: false
        description: |
          Whether to continue on with the workflow regardless of if another workflow is
          running
        type: boolean
      lock_key_env_var_name:
        default: WORKFLOW_LOCK_KEY
        description: |
          Environment variable containing the key of the workflow lock
          to acquire. This should generally be set in a Context.
        type: env_var_name
      poll_interval:
        default: 10
        description: Polling interval between attempts to continue the workflow (in
          seconds)
        type: integer
      ttl:
        default: 7 days
        description: |
          TTL of the workflow item in the store specified as a date string.
          See http://man7.org/linux/man-pages/man1/date.1.html#DATE_STRING for the format
          and https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html
          for how DynamoDB manages TTL.
        type: string
      wait_for:
        default: 240
        description: How long to wait before giving up (in minutes)
        type: integer
    steps:
    - run:
        command: |
          LOCK_KEY="${<< parameters.lock_key_env_var_name >>}"
          if [[ -z "$LOCK_KEY" ]]; then
            echo "No lock key set. Continuing..."
            exit 0
          fi

          MAX_ATTEMPTS="$(printf "%.0f" $((60 * << parameters.wait_for >> / << parameters.poll_interval >>)))"

          # Unix timestamp of when the commit was committed
          COMMITTED_AT="$(git log -1 --format=%ct)"

          # Create the DynamoDB key object and persist to the workspace for
          # downstream jobs to source
          WORKFLOW_KEY="$(
            echo '{}' \
              | jq --arg k "$LOCK_KEY" '.key.S = $k' - \
              | jq --arg t "$COMMITTED_AT" '.committed_at.N = $t' -)"
          mkdir -p /tmp/workspace
          echo "$WORKFLOW_KEY" > /tmp/workspace/workflow-key.json

          PREV_COMMIT="$(git rev-parse HEAD^)"

          # Create a temporary file containing the new item value (JSON)
          ITEM="$(mktemp)"
          echo '{}' \
            | jq --arg k "$LOCK_KEY" '.key.S = $k' - \
            | jq --arg t "$COMMITTED_AT" '.committed_at.N = $t' - \
            | jq --arg t "$(date +%s)" '.created_at.N = $t' - \
            | jq --arg t "$(date -d '<< parameters.ttl >>' +%s)" '.expires_at.N = $t' - \
            | jq --arg n "$CIRCLE_BUILD_NUM" '.build_num.N = $n' - \
            | jq --arg c "$CIRCLE_SHA1" '.commit.S = $c' - \
            | jq --arg u "${CIRCLE_USERNAME:-unknown}" '.username.S = $u' - \
            | jq --arg id "$CIRCLE_WORKFLOW_ID" '.workflow_id.S = $id' - \
            | jq '.status.S = "QUEUED"' - \
            | jq '.state.M = {}' - \
            > "$ITEM"
          cat "$ITEM"

          # Add the item to the table
          aws dynamodb put-item \
            --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
            --item "file://$ITEM"

          # Query the table to find the workflow with the single oldest committed_at
          # timestamp and with a status of running or queued
          function queryItems() {
            local values="$(
              echo '{":key": {"S": ""}, ":running": {"S": "RUNNING"}, ":queued": {"S": "QUEUED"}}' \
              | jq --arg k "$LOCK_KEY" '.[":key"].S = $k' -)"
            aws dynamodb query \
              --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
              --key-condition-expression '#key = :key' \
              --filter-expression '#status IN (:running, :queued)' \
              --expression-attribute-names '{"#key": "key", "#status": "status"}' \
              --expression-attribute-values "$values" \
              --max-items 1
          }

          function workflowForCommitExists() {
            (
              local commit="$1"
              local values="$(
                echo '{}' \
                | jq --arg k "$LOCK_KEY" '.[":key"].S = $k' - \
                | jq --arg c "$commit" '.[":commit"].S = $c' -)"
              local result="$(
                aws dynamodb query \
                  --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
                  --index-name commit \
                  --key-condition-expression '#key = :key AND #commit = :commit' \
                  --expression-attribute-names '{"#key": "key", "#commit": "commit"}' \
                  --expression-attribute-values "$values" \
                  --max-items 1)"
              local count="$(echo "$result" | jq -r .Count -)"
              if [[ "$count" == 1 ]]; then
                echo "Workflow exists for commit $commit"
                exit 0
              else
                echo "Workflow does not yet exist for commit $commit"
                exit 1
              fi
            )
          }

          <<# parameters.check_previous_commit >>
          export CHECK_PREVIOUS_COMMIT=1
          <</ parameters.check_previous_commit >>

          # Check if this workflow is at the front of the queue
          function isWorkflowFrontOfQueue() {
            (
              # Check that previous commit was processed. This means it has an item in
              # the workflows table.
              #
              # Merged commits are in order, but they race to the point of acquiring the
              # "lock". That race involves CircleCI spinning up a container in their
              # distributed system. Therefore a commit that happened second -- but by
              # only a few seconds -- could "win" the race to acquire the lock first.
              # This is a safety mechanism to ensure that merged comments are processed
              # in the correct order.
              if [[ -n "$CHECK_PREVIOUS_COMMIT" ]]; then
                if ! workflowForCommitExists "$PREV_COMMIT"; then
                  exit 1
                fi
              fi

              local result="$(queryItems)"
              if [[ $? -ne 0 ]]; then
                echo "Failed to query for workflow items"
                exit 1
              fi
              local workflow_id="$(echo "$result" | jq -r .Items[0].workflow_id.S -)"
              local count="$(echo "$result" | jq -r .Count -)"
              if [[ "$workflow_id" == "$CIRCLE_WORKFLOW_ID" || "$count" == 0 ]]; then
                echo "This workflow ($CIRCLE_WORKFLOW_ID) is front of the queue!"
                exit 0
              else
                echo "This workflow ($CIRCLE_WORKFLOW_ID) is not at the front of the queue. Next up: $(echo "$result" | jq .Items[0] -)"
                exit 1
              fi
            )
          }

          # Check if the commit is still the HEAD of the branch
          function isCommitHeadOfBranch() {
            (
              local head_sha="$(
                curl \
                  --user "${GITHUB_USERNAME}:${GITHUB_PASSWORD}" \
                  "https://api.github.com/repos/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}/branches/${CIRCLE_BRANCH}" \
                  --max-time 60 \
                  --connect-timeout 60 \
                | jq -r '.commit.sha' -)"
              [[ "$CIRCLE_SHA1" == "$head_sha" ]]
            )
          }

          # Set a flag based on the commit message that determines if this commit can be
          # skipped if there are other commits behind it in the queue. By default the
          # commit will be skipped
          SKIP_COMMIT_ALLOWED=1
          <<# parameters.do_not_cancel_workflow_if_tag_in_commit >>
          MESSAGE="$(git log -1 --pretty=%B)"
          shopt -s nocasematch
          if [[ "$MESSAGE" == *'<< parameters.do_not_cancel_workflow_if_tag_in_commit >>'* ]]; then
            SKIP_COMMIT_ALLOWED=0
          fi
          shopt -u nocasematch
          <</ parameters.do_not_cancel_workflow_if_tag_in_commit >>

          # Update the status of the workflow to RUNNING
          function updateWorkflowStatus() {
            local values="$(
              echo '{":status": {"S": "RUNNING"}}' \
                | jq --arg t "$(date +%s)" '.[":acquired_at"].N = $t' -)"

            aws dynamodb update-item \
              --table-name "$DYNAMODB_TABLE_WORKFLOWS" \
              --key "$WORKFLOW_KEY" \
              --update-expression 'SET #status = :status, #acquired_at = :acquired_at' \
              --expression-attribute-names '{"#status": "status", "#acquired_at": "acquired_at"}' \
              --expression-attribute-values "$values" \
              --return-values ALL_NEW
          }

          <<# parameters.force >>
          FORCE=1
          <</ parameters.force >>

          if [[ -n "$FORCE" ]]; then
            updateWorkflowStatus
            exit 0
          else
            n=1
            until [[ $n -gt "$MAX_ATTEMPTS" ]]; do
              echo "Attempt: $n of $MAX_ATTEMPTS"

              if [[ "$SKIP_COMMIT_ALLOWED" == 1 ]]; then
                if isCommitHeadOfBranch; then
                  echo "Commit $CIRCLE_SHA1 is still HEAD of the $CIRCLE_BRANCH branch"
                else
                  echo 'export CANCEL_JOB=1' >> $BASH_ENV
                  echo 'export WORKFLOW_LOCK_BUILD_STATUS=CANCELLED' >> $BASH_ENV
                  echo "Commit $CIRCLE_SHA1 is no longer HEAD of the $CIRCLE_BRANCH branch and will be skipped"
                  exit 0
                fi
              fi

              if isWorkflowFrontOfQueue; then
                updateWorkflowStatus
                exit 0
              fi

              sleep << parameters.poll_interval >>
              n=$[$n+1]
            done

            echo "Failed to acquire lock"
            exit 1
          fi
        name: Wait in a workflow queue until the job is at the front
    - persist_to_workspace:
        paths:
        - workflow-key.json
        root: /tmp/workspace
description: |
  Manage workflow concurrency and job state using an external store.
jobs:
  exit-queue:
    description: Exit the queue when a workflow is finished
    machine:
      enabled: true
    parameters:
      send_slack_on_recovery:
        default: false
        description: If true, the default channel will be notified on workflow recovery
        type: boolean
    steps:
    - aws-cli/install
    - exit-queue
    - when:
        condition: << parameters.send_slack_on_recovery >>
        steps:
        - send-slack-on-workflow-recovery
  wait-in-queue:
    description: Wait in a workflow queue until the job is at the front
    docker:
    - image: circleci/python:3.6.8
    parameters:
      check_previous_commit:
        default: false
        description: |
          Whether to check if the previous commit has been added to the queue before
          continuing with the workflow. This helps deal with the race condition of two
          commits getting merged seconds apart.
        type: boolean
      do_not_cancel_workflow_if_tag_in_commit:
        default: ""
        description: |
          Do not allow this workflow to self-cancel, even if it could have been ignored, if
          the provided tag is in the commit message (case-insensitive)
        type: string
      force:
        default: false
        description: |
          Whether to continue on with the workflow regardless of if another workflow is
          running
        type: boolean
      lock_key_env_var_name:
        default: WORKFLOW_LOCK_KEY
        description: |
          Environment variable containing the key of the workflow lock
          to acquire. This should generally be set in a Context.
        type: env_var_name
      poll_interval:
        default: 10
        description: Polling interval between attempts to continue the workflow (in
          seconds)
        type: integer
      ttl:
        default: 7 days
        description: |
          TTL of the workflow item in the store specified as a date string.
          See http://man7.org/linux/man-pages/man1/date.1.html#DATE_STRING for the format
          and https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html
          for how DynamoDB manages TTL.
        type: string
      wait_for:
        default: 240
        description: How long to wait before giving up (in minutes)
        type: integer
    steps:
    - checkout
    - aws-cli/install
    - wait-in-queue:
        check_previous_commit: << parameters.check_previous_commit >>
        do_not_cancel_workflow_if_tag_in_commit: << parameters.do_not_cancel_workflow_if_tag_in_commit
          >>
        force: << parameters.force >>
        lock_key_env_var_name: << parameters.lock_key_env_var_name >>
        poll_interval: << parameters.poll_interval >>
        ttl: << parameters.ttl >>
        wait_for: << parameters.wait_for >>
    - exit-queue:
        exit_condition: on_cancelled
    - cancel-job
orbs:
  aws-cli: circleci/aws-cli@0.1.16
  slack: circleci/slack@3.4.0
version: 2.1

